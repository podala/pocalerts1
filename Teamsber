import spacy
from spacy.training import Example
from spacy.util import minibatch, compounding
from spacy.pipeline import EntityRuler
import random

# Load a pre-trained English model
nlp = spacy.load("en_core_web_sm")

# Create an EntityRuler to add explicit rules (optional but recommended for hard cases)
ruler = EntityRuler(nlp)
patterns = [
    {"label": "USER", "pattern": "Alice"},
    {"label": "USER", "pattern": "Bob"},
    {"label": "TEAM", "pattern": "team_alpha"},
    {"label": "TEAM", "pattern": "team_beta"}
]
ruler.add_patterns(patterns)
nlp.add_pipe(ruler, before="ner")

# Define templates and sample data
templates = [
    "Create team {team} with users {users}",
    "Set up team {team} including members {users}",
    "Establish a new team named {team} with users {users}",
]
teams = ["team_alpha", "team_beta", "team_gamma"]
user_groups = [["Alice", "Bob"], ["Charlie", "Dave"], ["Eve", "Frank"]]

# Generate training data
train_data = []
for template in templates:
    for team in teams:
        for users in user_groups:
            user_str = ', '.join(users)
            sentence = template.format(team=team, users=user_str)
            entities = []
            team_start = sentence.find(team)
            team_end = team_start + len(team)
            entities.append((team_start, team_end, 'TEAM'))
            # Calculate user entities
            offset = sentence.find(user_str)
            for user in users:
                user_start = sentence.find(user, offset)
                user_end = user_start + len(user)
                entities.append((user_start, user_end, 'USER'))
                offset = user_end + 1
            train_data.append((sentence, {'entities': entities}))

# Training the NER model
with nlp.disable_pipes(*[pipe for pipe in nlp.pipe_names if pipe not in ['ner', 'ruler']]):  # Only train NER and use Ruler
    optimizer = nlp.initialize()
    for i in range(10):  # Number of training iterations
        random.shuffle(train_data)
        losses = {}
        for batch in minibatch(train_data, size=compounding(4., 32., 1.001)):
            for text, annotations in batch:
                doc = nlp.make_doc(text)
                example = Example.from_dict(doc, annotations)
                nlp.update([example], drop=0.5, sgd=optimizer, losses=losses)
        print(f"Iteration {i}, Losses: {losses}")

# Save the trained model
nlp.to_disk("./team_user_ner_model")

# Function to process input and extract entities
def process_input(statement):
    doc = nlp(statement)
    return [(ent.text, ent.label_) for ent in doc.ents]

# Test the model with an example
example_statement = "Create team team_alpha with users Alice, Bob"
print(process_input(example_statement))
