import os
import random
import numpy as np
from sentence_transformers import SentenceTransformer, InputExample, losses
from torch.utils.data import DataLoader
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sklearn.metrics import mean_squared_error, pearsonr, cosine_similarity
import nltk
from nltk.corpus import wordnet as wn
from nltk.stem import WordNetLemmatizer

nltk.download('punkt')
nltk.download('wordnet')

# Initialize a lemmatizer
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    """ Basic text preprocessing to lemmatize and lower case """
    tokens = nltk.word_tokenize(text.lower())
    lemmatized = [lemmatizer.lemmatize(token) for token in tokens]
    return ' '.join(lemmatized)

def augment_sentence(sentence, synonym_ratio=0.3):
    """ Augment sentence by replacing some words with their synonyms """
    words = nltk.word_tokenize(sentence)
    new_words = []
    for word in words:
        if random.random() < synonym_ratio:
            synonyms = [lem.name().replace('_', ' ') for syn in wn.synsets(word) for lem in syn.lemmas()]
            if synonyms:
                new_words.append(random.choice(synonyms))
                continue
        new_words.append(word)
    return ' '.join(new_words)

# Example dataset of notes
notes = [
    "Discharge plan must be set",
    "Need to reach member to monitor the discharge",
    "Member reached out for consultation",
    "Follow discharge protocol",
    "I need to contact member for further instructions"
]

# Augment data
augmented_notes = [augment_sentence(note) for note in notes for _ in range(5)]  # Augment each note 5 times

# Preprocess notes
processed_notes = [preprocess_text(note) for note in notes + augmented_notes]

# Load a pre-trained model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Prepare training data
train_examples = [InputExample(texts=[note], label=0.0) for note in processed_notes]
train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)
train_loss = losses.MultipleNegativesRankingLoss(model)

# Prepare evaluator with some annotations (example)
annotations = [
    (0, 1, 0.3),
    (0, 2, 0.1),
    (1, 4, 0.9),
    (2, 3, 0.4),
    (3, 4, 0.2)
]

def evaluate_embeddings(embeddings, annotations):
    """Calculates and prints evaluation metrics for embeddings."""
    cos_sims = []
    human_sims = []
    for idx1, idx2, human_sim in annotations:
        cos_sim = cosine_similarity([embeddings[idx1]], [embeddings[idx2]])[0][0]
        cos_sims.append(cos_sim)
        human_sims.append(human_sim)

    mse = mean_squared_error(human_sims, cos_sims)
    pearson_corr = pearsonr(cos_sims, human_sims)[0]
    spearman_corr = np.corrcoef(cos_sims, human_sims)[0, 1]
    print("MSE:", mse, "Pearson Correlation:", pearson_corr, "Spearman's rho:", spearman_corr)

# Train and evaluate model
model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, output_path='trained_model')

# Encode embeddings for evaluation
note_embeddings = np.array([model.encode(note).tolist() for note in processed_notes])
evaluate_embeddings(note_embeddings, annotations)

# Function to search notes using semantic similarity
def search_notes_semantic(query, model, notes):
    query_embedding = model.encode(query)
    cosine_scores = cosine_similarity([query_embedding], note_embeddings)[0]
    sorted_indices = cosine_scores.argsort()[::-1]
    return [(notes[index], cosine_scores[index]) for index in sorted_indices]

# Main execution
if __name__ == "__main__":
    query = "I need to reach member"
    results = search_notes_semantic(query, model, notes + augmented_notes)
    # Print out the results
    print("Top matching notes for query:", query)
    for note, score in results:
        print(f"Content: {note}, Score: {score:.4f}\n")
