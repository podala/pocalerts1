

## üîπ PHASE 1: Baseline Measurement (MANDATORY)

### Step 1.1 ‚Äì Capture table physical stats

```sql
DESCRIBE DETAIL curated.sales;
```

Capture and store:

* `numFiles`
* `sizeInBytes`
* `partitionColumns`
* `createdAt`
* `lastModified`

üìå **Decision rules**

| Evidence          | Threshold | Meaning             |
| ----------------- | --------- | ------------------- |
| numFiles > 10,000 | ‚ùå         | Small file problem  |
| avg file < 100 MB | ‚ùå         | Compaction required |
| No partitions     | ‚ö†Ô∏è        | Expect full scans   |

---

### Step 1.2 ‚Äì Capture transaction health

```sql
DESCRIBE HISTORY curated.sales;
```

Check:

* Frequency of `MERGE`
* Overwrites
* Optimize history

üìå **Decision rules**

| Evidence               | Meaning                   |
| ---------------------- | ------------------------- |
| Many MERGEs daily      | Partition + ZORDER needed |
| Frequent overwrites    | Log growth risk           |
| No OPTIMIZE in 30 days | Likely degraded           |

---

### Step 1.3 ‚Äì Capture real query behavior (proof)

Run **actual production query** with:

```sql
EXPLAIN ANALYZE
SELECT ...
```

From Spark UI / SQL UI capture:

* Files scanned
* Bytes read
* Duration
* Shuffle size

üìå **Decision rules**

| Evidence                    | Meaning                   |
| --------------------------- | ------------------------- |
| Files scanned ‚âà total files | Data skipping ineffective |
| Bytes read ‚â´ result size    | Partition/ZORDER needed   |
| Shuffle > 30% time          | Partition skew            |

---

## üîπ PHASE 2: Decide Required Optimizations (RULE BASED)

### 2.1 Small file decision

If:

* `avgFileSize < 100MB`
* OR `numFiles > 10k`

‚úÖ **Action**

```sql
OPTIMIZE curated.sales;
```

üìå Proof = avg file size increases + fewer files.

---

### 2.2 Partition decision (NOT optional)

Look at query filters:

```sql
WHERE clientId = ?
AND year = ?
AND month = ?
```

üìå **Rule**

* Partition ONLY on columns used in **90% of filters**
* Cardinality < 10k

‚úÖ **Action**

```python
df.write.partitionBy("clientId","year","month")
```

‚ùå Never repartition blindly.

---

### 2.3 Z-ORDER decision (very strict)

Check query profile:

* Data skipped < 40%

‚úÖ **Only then**

```sql
OPTIMIZE curated.sales
ZORDER BY (productId, region);
```

üìå Proof = Files scanned drops.

If skipped > 80% ‚Üí **DO NOT ZORDER**

---

### 2.4 MERGE optimization decision

If:

* MERGE joins on `id + year`
* Shuffle is large

‚úÖ **Action**

* Ensure join keys are:

  * Partition keys OR
  * Z-ORDER keys

Otherwise MERGE cost will grow **linearly forever**.

---

### 2.5 Metadata cleanup decision

Check:

```bash
ls _delta_log | wc -l
```

üìå Rules:

| Evidence        | Action              |
| --------------- | ------------------- |
| > 10k logs      | VACUUM              |
| Slow table open | Increase checkpoint |

‚úÖ Action:

```sql
VACUUM curated.sales RETAIN 168 HOURS;
SET spark.databricks.delta.checkpointInterval = 10;
```

---

## üîπ PHASE 3: Execute Optimizations (CONTROLLED)

### Execution order (DO NOT CHANGE)

1Ô∏è‚É£ OPTIMIZE (files)
2Ô∏è‚É£ ZORDER (if justified)
3Ô∏è‚É£ VACUUM
4Ô∏è‚É£ ANALYZE TABLE

```sql
OPTIMIZE curated.sales ZORDER BY (productId);
VACUUM curated.sales RETAIN 168 HOURS;
ANALYZE TABLE curated.sales COMPUTE STATISTICS;
```

---

## üîπ PHASE 4: Post-Proof Validation (NON-NEGOTIABLE)

Re-run **same query**.

Capture:

* Files scanned (before vs after)
* Bytes read
* Duration

üìä **Expected minimum gain**

| Metric        | Expected |
| ------------- | -------- |
| Files scanned | ‚Üì 70‚Äì95% |
| Query time    | ‚Üì 50‚Äì90% |
| Shuffle       | ‚Üì 60%    |

If no gain ‚Üí rollback ZORDER (wasted cost).

---

## üîπ PHASE 5: Automate Decision

Create **Delta Audit Table**:

```sql
performance_audit (
  table_name,
  run_date,
  num_files,
  avg_file_mb,
  files_scanned,
  duration_sec,
  optimize_needed BOOLEAN,
  zorder_needed BOOLEAN
)
```

Schedule weekly job:

* Measure
* Decide
* Optimize **only if thresholds breached**

---
