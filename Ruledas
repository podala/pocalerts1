

### **✅ Goal**

Fetch all expressions (like `shifted_maf_adj_exprs`) dynamically from a database, so analysts or admins can update logic without redeploying code.

---

### **1. Define a configuration table**

Create a table, say `formula_repository`, to store dynamic Spark SQL expressions and metadata.

| Column name       | Type      | Description                               |
| ----------------- | --------- | ----------------------------------------- |
| `rule_id`         | STRING    | Unique key (e.g. `SHIFTED_MAF_ADJ`)       |
| `rule_name`       | STRING    | Human-readable name                       |
| `rule_expression` | STRING    | The Spark SQL CASE or arithmetic formula  |
| `is_active`       | BOOLEAN   | Whether rule is active                    |
| `year_token`      | STRING    | Placeholder (e.g. `{y}` for substitution) |
| `last_updated`    | TIMESTAMP | For version tracking                      |
| `remarks`         | STRING    | Optional comments                         |

Example row:

```sql
INSERT INTO formula_repository VALUES (
  'SHIFTED_MAF_ADJ',
  'Shifted MAF Adjustment Formula',
  'CASE WHEN COALESCE(Selected_Specialty_Ind, '''') = ''Y'' ... END AS Year{y}',
  TRUE,
  '{y}',
  current_timestamp(),
  'Base formula for MAF adjustment across years'
);
```

---

### **2. Read the formula from DB**

In your PySpark job:

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.getOrCreate()

# Read from database (using JDBC)
formula_df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", "formula_repository") \
    .option("user", db_user) \
    .option("password", db_pass) \
    .load()

# Filter the desired formula
rule_row = formula_df.filter("rule_id = 'SHIFTED_MAF_ADJ'").collect()[0]
formula_template = rule_row["rule_expression"]
```

---

### **3. Generate year-wise expressions dynamically**

```python
years = [1, 2, 3, 4, 5]
shifted_maf_adj_exprs = [
    formula_template.replace("{y}", str(y)) for y in years
]
```

---

### **4. Execute dynamically in Spark**

```python
shifted_maf_adj_df = df.selectExpr(*all_columns, *shifted_maf_adj_exprs)
```

---

### **5. Versioning & audit trail**

* Keep a `formula_version` column in the table.
* Maintain a `formula_history` table for previous versions.
* Add `updated_by` and `approved_by` for governance.
* Optionally tag formulas by `client_id`, `opportunity_id`, etc., if underwriter logic differs per client.

---

### **6. Optional enhancements**

✅ Store constants (e.g. `bconst`) in a `parameter_repository` table
✅ Use a **cache** (broadcast join or in-memory dict) for constant retrieval
✅ Add a **UI layer** (React form) for analysts to update formulas safely
✅ Implement **lint validation** in Python before formula execution (to ensure valid Spark SQL syntax)

---

### **Example End-to-End Code Block**

```python
# Step 1: Read formula from DB
formula_df = spark.read.format("jdbc") \
    .option("url", jdbc_url) \
    .option("dbtable", "formula_repository") \
    .option("user", db_user) \
    .option("password", db_pass) \
    .load()

rule_expr = formula_df.filter("rule_id = 'SHIFTED_MAF_ADJ'").collect()[0]["rule_expression"]

# Step 2: Replace placeholder for each year
years = [1,2,3,4,5]
shifted_maf_adj_exprs = [rule_expr.replace("{y}", str(y)) for y in years]

# Step 3: Execute dynamically
shifted_maf_adj_df = df.selectExpr(*all_columns, *shifted_maf_adj_exprs)
```

---

### **Benefits**

* ✅ No code redeployment for logic changes.
* ✅ Supports per-client or per-model rule variations.
* ✅ Enables metadata-driven governance and auditing.
* ✅ Fully scalable for 100+ formulas.

---

Would you like me to generate the **SQL schema + sample insert script + PySpark connector (with retry + broadcast caching)** as a production-ready version next?
