import spacy
from spacy.training import Example
import random
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Data Generation for Annotations
def generate_and_annotate_data(n):
    operations = ["Add", "Remove"]
    users = ["000678689", "000456789", "000345678", "smohit", "spodala"]
    teams = ["teamalpha", "teambeta", "teamgamma"]

    data = []
    for _ in range(n):
        operation = random.choice(operations)
        selected_users = random.sample(users, k=random.randint(1, 3))  # Select 1-3 users randomly
        team = random.choice(teams)
        user_part = ', '.join(selected_users)
        sentence = f"{operation} users {user_part} to team {team}"

        annotations = {'entities': []}
        annotations['entities'].append((sentence.find(operation), sentence.find(operation) + len(operation), 'OPERATION'))
        annotations['entities'].append((sentence.find(team), sentence.find(team) + len(team), 'TEAM'))
        for user in selected_users:
            start_idx_user = sentence.find(user)
            end_idx_user = start_idx_user + len(user)
            annotations['entities'].append((start_idx_user, end_idx_user, 'USER'))

        data.append((sentence, annotations))
    return data

# Step 2: Setup spaCy Model and Pipeline
nlp = spacy.blank('en')
if 'ner' not in nlp.pipe_names:
    ner = nlp.add_pipe('ner', last=True)
for label in ['OPERATION', 'USER', 'TEAM']:
    ner.add_label(label)

# Step 3: Prepare Training and Test Data
data = generate_and_annotate_data(200)  # Generate 200 examples
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)

# Convert training data to Example objects
train_examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in train_data]

# Step 4: Train the Model
nlp.initialize()
for i in range(10):
    random.shuffle(train_examples)
    losses = {}
    for batch in spacy.util.minibatch(train_examples, size=4):
        nlp.update(batch, drop=0.5, losses=losses)
    print(f"Losses at iteration {i}: {losses}")

# Step 5: Evaluate the Model
def evaluate_model(nlp, test_data):
    test_examples = [Example.from_dict(nlp.make_doc(text), annot) for text, annot in test_data]
    scorer = nlp.evaluate(test_examples)
    print(f"Precision: {scorer['ents_p']}")
    print(f"Recall: {scorer['ents_r']}")
    print(f"F1-score: {scorer['ents_f']}")

    # Prepare data for confusion matrix
    labels = ['OPERATION', 'USER', 'TEAM']
    true_labels = []
    pred_labels = []
    for example in test_examples:
        true = [span.label_ for span in example.reference.ents]
        doc = nlp(example.reference.text)
        pred = [span.label_ for span in doc.ents]
        true_labels.extend(true)
        pred_labels.extend(pred)

    cm = confusion_matrix(true_labels, pred_labels, labels=labels)
    print(classification_report(true_labels, pred_labels, labels=labels))
    return cm, labels

cm, labels = evaluate_model(nlp, test_data)

# Step 6: Plot Confusion Matrix
def plot_confusion_matrix(cm, labels, title='Confusion Matrix', cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(labels))
    plt.xticks(tick_marks, labels, rotation=45)
    plt.yticks(tick_marks, labels)
    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()

plot_confusion_matrix(cm, labels)

# Step 7: Prediction Function
def predict_entities(text):
    doc = nlp(text)
    return [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

# Example Prediction
example_text = "Add users 000678689, smohit to team teamalpha"
predictions = predict_entities(example_text)
print("Predictions:", predictions)
