import pandas as pd
import json
import logging
from collections import defaultdict

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# Configuration for different header groups
config = {
    "header_groups": {
        "PL_Information": {
            "structure_type": "default",
            "pattern": "flat",
            "headers": ["PL Information", "Rpt Information"]
        },
        "Scripts": {
            "structure_type": "hierarchical",
            "pattern": "header-categories-subCategories-yearlyData",
            "headers": ["Scripts", "Eligibility Scripts"],
            "categories": ["Mail", "Retail 30", "Specialty"],
            "subCategories": ["Brand", "Generic", "Total"]
        }
    }
}

# JSON Templates for Structure Validation
json_templates = {
    "flat": {
        "PL_group": "",
        "vector": "",
        "PL_ID": "",
        "PL Run Date": "",
        "modelYears": []
    },
    "header-categories-subCategories-yearlyData": {
        "headerName": "",
        "categories": []
    }
}

# Base Processor Class
class BaseProcessor:
    def __init__(self, group_name, config):
        self.group_name = group_name
        self.config = config
        self.structure_type = config["header_groups"][group_name]["structure_type"]
        self.template = json_templates[self.config["header_groups"][group_name]["pattern"]]
        logging.info(f"Initialized Processor for {group_name}")

    def process_row(self, row):
        raise NotImplementedError("Each processor must implement its own row processing")

# Processor for PL Information
class PLInformationProcessor(BaseProcessor):
    def process_row(self, row):
        logging.info(f"Processing PL_Information row: {row}")
        
        processed_documents = []
        
        for vector_index in range(1, 6):  # Vectors 1 to 5
            document = self.template.copy()
            document["PL_group"] = "PL_Information"
            document["vector"] = vector_index
            document["PL_ID"] = row.get("PL ID", "N/A")
            document["PL Run Date"] = row.get("PL Run Date", "N/A")
            
            # Extract model years dynamically
            document["modelYears"] = []
            for i in range(1, 6):
                year_key = f"vector {vector_index}"
                if year_key in row and pd.notna(row[year_key]):
                    document["modelYears"].append({"year": int(row[year_key])})

            processed_documents.append(document)
        
        return processed_documents

# Unified Processor for Scripts and Eligibility Scripts
class ScriptsProcessor(BaseProcessor):
    def process_row(self, row):
        logging.info(f"Processing Scripts row: {row}")
        
        document = self.template.copy()
        document["headerName"] = row["header"]
        categories_dict = defaultdict(lambda: defaultdict(list))

        for category in self.config["header_groups"]["Scripts"]["categories"]:
            for subCategory in self.config["header_groups"]["Scripts"]["subCategories"]:
                column_key = f"{category} - {subCategory}"
                if column_key in row:
                    for i in range(1, 6):
                        year_key = f"Year {i}"
                        if year_key in row and pd.notna(row[year_key]):
                            categories_dict[category][subCategory].append({
                                "year": int(row[year_key]),
                                "value": float(row[column_key])
                            })

        for category, subCategories in categories_dict.items():
            category_obj = {"categoryName": category, "subCategories": []}
            for subName, yearly_data in subCategories.items():
                category_obj["subCategories"].append({"subCategoryName": subName, "yearlyData": yearly_data})
            document["categories"].append(category_obj)

        return document

# Processing Engine
def process_csv(csv_file, config):
    logging.info("Starting CSV Processing")
    df = pd.read_csv(csv_file, chunksize=1000)  # Process in chunks for large files
    processors = {
        "PL_Information": PLInformationProcessor("PL_Information", config),
        "Scripts": ScriptsProcessor("Scripts", config)
    }
    processed_documents = []
    
    for chunk in df:
        logging.info(f"Processing chunk with {len(chunk)} rows")
        for _, row in chunk.iterrows():
            group_name = row["header"]
            logging.info(f"Processing row under group {group_name}")
            if group_name in processors:
                document = processors[group_name].process_row(row)
                if document:
                    if isinstance(document, list):  # For multiple vectors
                        processed_documents.extend(document)
                    else:
                        processed_documents.append(document)

    logging.info(f"Total documents processed: {len(processed_documents)}")
    with open("output.json", "w") as f:
        json.dump(processed_documents, f, indent=4)
    logging.info("Saved output.json successfully")

if __name__ == "__main__":
    csv_file = "input_data.csv"
    logging.info("Starting Main Execution")
    process_csv(csv_file, config)
