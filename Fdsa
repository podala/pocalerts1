Perfect. Now letâ€™s design a **production-grade MongoDB Atlas RAG workflow** for *your underwriting / regulatory use case*, combining:

> âœ… Semantic search
> âœ… Metadata filtering
> âœ… Hybrid search (vector + keyword)
> âœ… Business-aware re-ranking
> âœ… Version + governance control
> âœ… Audit trail

This will be fully aligned with your enterprise underwriting system.

---

# ðŸŽ¯ YOUR USE CASE CONTEXT

You have:

* State regulations
* Contract documents
* Rebate clauses
* Versioned policy docs
* Client-specific documents
* Legal approvals
* Effective dates

And you need:

* Accurate retrieval
* Version control
* Tenant isolation
* Regulatory correctness
* Traceability

---

# ðŸ— E2E MONGODB RAG WORKFLOW

---

# ðŸ§± STEP 1 â€” DOCUMENT INGESTION

Collection: `knowledge_chunks`

Example document:

```json
{
  "_id": "chunk_abc_001",
  "documentId": "regulation_2026_v3",
  "clientId": "ABC",
  "sourceType": "state_regulation",
  "sourceName": "NY State Regulation",
  "version": "2026.1",
  "effectiveDate": ISODate("2026-01-01"),
  "pageNumber": 14,
  "section": "Rebate Cap Clause",
  "text": "The rebate shall not exceed...",
  "embedding": [ ... ],
  "approved": true,
  "confidenceScore": 0.98,
  "lastReviewedBy": "LegalTeam"
}
```

---

# ðŸ§± STEP 2 â€” INDEXING

## Vector Index

```json
{
  "fields": [
    {
      "type": "vector",
      "path": "embedding",
      "numDimensions": 1536,
      "similarity": "cosine"
    }
  ]
}
```

---

## Full-Text Search Index (Hybrid Support)

Atlas Search index:

```json
{
  "mappings": {
    "dynamic": false,
    "fields": {
      "text": { "type": "string" },
      "section": { "type": "string" }
    }
  }
}
```

---

# ðŸ§± STEP 3 â€” USER QUERY FLOW

User asks:

> "What is the rebate cap for NY regulation effective 2026?"

---

# ðŸ§  STEP 4 â€” QUERY PROCESSING

## 4.1 Generate embedding for query

```
queryEmbedding = embed(userQuery)
```

---

## 4.2 HYBRID + FILTERED SEARCH PIPELINE

```js
db.knowledge_chunks.aggregate([

  // 1ï¸âƒ£ Vector Search (semantic)
  {
    $vectorSearch: {
      index: "vector_index",
      path: "embedding",
      queryVector: queryEmbedding,
      numCandidates: 200,
      limit: 50,
      filter: {
        approved: true,
        clientId: "ABC",
        effectiveDate: { $lte: new Date() }
      }
    }
  },

  // 2ï¸âƒ£ Add vector score
  {
    $addFields: {
      vectorScore: { $meta: "vectorSearchScore" }
    }
  },

  // 3ï¸âƒ£ Full-Text Keyword Boost (hybrid layer)
  {
    $addFields: {
      keywordBoost: {
        $cond: [
          { $regexMatch: { input: "$text", regex: "rebate cap", options: "i" } },
          1,
          0
        ]
      }
    }
  },

  // 4ï¸âƒ£ Business Re-Ranking
  {
    $addFields: {
      finalScore: {
        $add: [
          { $multiply: ["$vectorScore", 0.6] },
          { $multiply: ["$confidenceScore", 0.2] },
          { $multiply: ["$keywordBoost", 0.1] },
          {
            $multiply: [
              { $cond: ["$approved", 1, 0] },
              0.1
            ]
          }
        ]
      }
    }
  },

  { $sort: { finalScore: -1 } },
  { $limit: 5 }
])
```

---

# ðŸ’¡ What Just Happened?

You combined:

| Layer           | Purpose                    |
| --------------- | -------------------------- |
| Vector search   | Semantic meaning           |
| Metadata filter | Tenant + version control   |
| Keyword boost   | Precision on legal phrases |
| ConfidenceScore | Legal review weight        |
| Approved flag   | Governance enforcement     |
| FinalScore sort | Business-aware ranking     |

This is exactly how production systems combine:

> semantic + filtering + hybrid + ranking

---

# ðŸ§  STEP 5 â€” CONTEXT BUILDING

Top 5 chunks are passed to LLM:

```
[1] NY Regulation 2026.1 Page 14
"The rebate shall not exceed..."

[2] NY Regulation 2026.1 Page 15
...
```

---

# ðŸ§  STEP 6 â€” LLM GENERATION

Prompt:

```
Answer strictly based on the following regulatory excerpts.
Cite using [1], [2], etc.
Do not invent.
```

---

# ðŸ§  STEP 7 â€” CITATION MAPPING

Store:

```json
{
  "question": "...",
  "retrievedChunkIds": ["chunk_abc_001", "chunk_abc_014"],
  "finalAnswer": "...",
  "citations": {
    "1": "chunk_abc_001"
  },
  "timestamp": ISODate()
}
```

---

# ðŸ›¡ STEP 8 â€” GUARDRAILS

Before sending to user:

âœ” If no chunk score > 0.75 â†’ fallback
âœ” If chunks from mixed versions â†’ reject
âœ” If no approved document â†’ reject

---

# ðŸ“Š STEP 9 â€” FEEDBACK LOOP

Collection: `rag_feedback`

```json
{
  "questionId": "...",
  "chunkId": "chunk_abc_001",
  "feedback": "incorrect",
  "user": "underwriter_1",
  "timestamp": ...
}
```

Adjust future `confidenceScore`.

---

# ðŸ¢ ENTERPRISE-READY ARCHITECTURE

```
User Query
   â†“
Embedding
   â†“
Vector Search (semantic)
   â†“
Metadata Filtering (client + version + approved)
   â†“
Keyword Boost (hybrid)
   â†“
Business Re-Ranking
   â†“
Top-K Chunks
   â†“
LLM Answer
   â†“
Citation Mapping
   â†“
Audit Log
   â†“
User
```

---

# ðŸš€ WHY THIS IS INDUSTRIAL GRADE

Because it:

âœ” Prevents cross-client data leaks
âœ” Prevents outdated regulation usage
âœ” Prioritizes legally reviewed content
âœ” Combines semantic + lexical precision
âœ” Stores full traceability
âœ” Enables deterministic re-ranking

---

# âš™ PERFORMANCE CONSIDERATION

For scale (10M+ chunks):

* Use Search Nodes
* Use sharding by clientId
* Limit numCandidates
* Project only required fields
* Cache embeddings

---

# ðŸ”¥ Final Answer

In production MongoDB RAG systems, teams combine:

> Semantic search

* Strict metadata filtering
* Hybrid keyword boosting
* Weighted business re-ranking
* Governance validation

Exactly what we designed above.

---

If you want next level:

* I can show cost + latency modeling
* Or multi-tenant scaling design
* Or cross-encoder re-ranking integration
* Or show difference vs Azure AI Search

Tell me which direction you want to go deeper.

