import spacy
from spacy.training import Example
import random
import matplotlib.pyplot as plt
import json
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# Function to preprocess text input
def preprocess_text(text):
    # Replace multiple spaces with a single space and strip leading/trailing spaces
    text = ' '.join(text.split())
    # Correct common spelling mistakes
    corrections = {'remov': 'remove', 'delet': 'delete', 'addd': 'add', 'insertt': 'insert'}
    for wrong, right in corrections.items():
        text = text.replace(wrong, right)
    return text

# Data augmentation for diversity
def augment_data(sentence):
    synonyms = {
        "add": ["add", "insert", "append"],
        "remove": ["remove", "delete", "eliminate"]
    }
    for key, words in synonyms.items():
        if key in sentence:
            sentence = sentence.replace(key, random.choice(words))
            break
    return sentence

# Generate synthetic annotated data
def generate_data(n):
    actions = ["add", "remove"]
    names = ["john_doe", "jane_doe", "mohit", "alice_w"]
    ids = ["99999999", "6678796", "54678909"]
    teams = ["teamAlpha", "teamBeta", "teamGamma"]
    data = []
    for _ in range(n):
        action = random.choice(actions)
        num_users = random.randint(1, 3)
        user_list = random.sample(names + ids, k=num_users)
        team = random.choice(teams)
        user_text = ", ".join(user_list)
        sentence = f"{action} users {user_text} to {team}"
        sentence = preprocess_text(sentence)  # Preprocess the generated sentence
        augmented_sentence = augment_data(sentence)
        entities = []
        action_idx = augmented_sentence.find(action)
        entities.append((action_idx, action_idx + len(action), "ACTION"))
        start_idx = augmented_sentence.find(user_text)
        for user in user_list:
            user_start = augmented_sentence.find(user, start_idx)
            user_end = user_start + len(user)
            entities.append((user_start, user_end, "USER"))
            start_idx = user_end + 2
        team_start = augmented_sentence.find(team, start_idx)
        team_end = team_start + len(team)
        entities.append((team_start, team_end, "TEAM"))
        data.append((augmented_sentence, {"entities": entities}))
    return data

# Initialize and train spaCy model
nlp = spacy.blank("en")
ner = nlp.add_pipe("ner", last=True)
for label in ["USER", "TEAM", "ACTION"]:
    ner.add_label(label)

data = generate_data(100)
train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
train_examples = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in train_data]

# Training
nlp.initialize(lambda: train_examples)
for i in range(10):
    random.shuffle(train_examples)
    losses = {}
    nlp.update(train_examples, drop=0.5, losses=losses)
    print(f"Epoch {i + 1}, Losses: {losses}")

# Prediction function
def predict_entities(text):
    text = preprocess_text(text)  # Preprocess the input text
    doc = nlp(text)
    result = {'operation': None, 'users': [], 'team': None}
    for ent in doc.ents:
        if ent.label_ == 'USER':
            result['users'].append(ent.text)
        elif ent.label_ == 'TEAM':
            result['team'] = ent.text
        elif ent.label_ == 'ACTION':
            result['operation'] = ent.text
    return json.dumps(result, indent=4)

# Example usage
example_text = "remov  users john_doe, 99999999  to  teamBeta"
print(predict_entities(example_text))
