import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from torchcrf import CRF

# Define the NER Dataset
class NERDataset(Dataset):
    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):
        self.sentences = [torch.tensor([word_to_ix.get(word, word_to_ix['<UNK>']) for word in sentence], dtype=torch.long) for sentence in sentences]
        self.tags = [torch.tensor([tag_to_ix[tag] for tag in tag_list], dtype=torch.long) for tag_list in tags]

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return self.sentences[idx], self.tags[idx]

# Define the collate function to handle padding
def pad_collate(batch):
    sentences, tags = zip(*batch)
    sentences_padded = pad_sequence(sentences, batch_first=True)
    tags_padded = pad_sequence(tags, batch_first=True, padding_value=-1)  # Use -1 for padding value
    return sentences_padded, tags_padded

# Define the BiLSTM-CRF model
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, 50)
        self.lstm = nn.LSTM(50, 32, num_layers=1, bidirectional=True, batch_first=True)
        self.hidden2tag = nn.Linear(64, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, sentences):
        embeds = self.embedding(sentences)
        lstm_out, _ = self.lstm(embeds)
        lstm_feats = self.hidden2tag(lstm_out)
        return lstm_feats

    def loss(self, sentences, tags):
        feats = self.forward(sentences)
        return -self.crf(feats, tags)

# Training function
def train(model, device, train_loader, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for sentences, tags in train_loader:
            sentences, tags = sentences.to(device), tags.to(device)
            optimizer.zero_grad()
            loss = model.loss(sentences, tags)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}: Loss {total_loss / len(train_loader)}")

# Example usage
def main():
    # Sample vocabulary and tag set (for demonstration purposes)
    word_to_ix = {'hello': 1, 'world': 2, '<UNK>': 0}
    tag_to_ix = {'B-GREET': 0, 'I-GREET': 1, '<PAD>': -1}

    # Sample data
    sentences = [['hello', 'world'], ['world', 'hello']]
    tags = [['B-GREET', 'I-GREET'], ['B-GREET', 'I-GREET']]

    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Initialize dataset and dataloader
    dataset = NERDataset(sentences, tags, word_to_ix, tag_to_ix)
    train_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=pad_collate)

    # Initialize model and optimizer
    model = BiLSTM_CRF(len(word_to_ix), len(tag_to_ix)).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    # Train the model
    train(model, device, train_loader, optimizer)

if __name__ == "__main__":
    main()
