import pandas as pd
import json
import logging
from collections import defaultdict
from functools import wraps

# Configure Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# YAML-like Config for Defining Groups and Structure Patterns
config = {
    "header_groups": {
        "PL_Information": {
            "structure_type": "default",
            "pattern": "flat",
            "headers": ["PL Information", "Rpt Information"]
        },
        "Scripts": {
            "structure_type": "hierarchical",
            "pattern": "header-categories-subCategories-yearlyData",
            "headers": ["Scripts", "Eligibility Scripts"],
            "categories": ["Mail", "Retail 30", "Specialty"],
            "subCategories": ["Brand", "Generic", "Total"]
        }
    }
}

# Retry Decorator for Fault Tolerance
def retry_on_failure(retries=3, delay=2):
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            attempt = 0
            while attempt < retries:
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    attempt += 1
                    logging.error(f"Error in {func.__name__}: {str(e)}. Retrying {attempt}/{retries}")
            logging.error(f"Max retries reached for {func.__name__}. Skipping.")
        return wrapper
    return decorator

# Base Processor Class
class BaseProcessor:
    def __init__(self, group_name, config, output_file):
        self.group_name = group_name
        self.config = config
        self.structure_type = config["header_groups"][group_name]["structure_type"]
        self.pattern = config["header_groups"][group_name]["pattern"]
        self.output_file = output_file
        self.processed_documents = []
        logging.info(f"Initialized Processor for {group_name}")

    def process_row(self, row):
        raise NotImplementedError("Each processor must implement its own row processing")

    @retry_on_failure()
    def save_to_file(self):
        """ Save processed JSON output to file """
        if self.processed_documents:
            with open(self.output_file, "w") as f:
                json.dump(self.processed_documents, f, indent=4)
            logging.info(f"Saved {len(self.processed_documents)} documents to {self.output_file}")

# Processor for PL Information
class PLInformationProcessor(BaseProcessor):
    def process_row(self, row):
        logging.info(f"Processing PL_Information row: {row}")

        document = {
            "PL_group": "PL_Information",
            "vector": row["vector"],
            "modelYears": []
        }

        # Extracting all key-value pairs dynamically
        for column, value in row.items():
            if "Model Year" in column and isinstance(value, (int, float, str)) and str(value).isdigit():
                document["modelYears"].append({"year": int(value)})
            else:
                document[column] = value  # Store all other KPI values as key-value pairs

        self.processed_documents.append(document)

# Processor for Scripts & Eligibility Scripts
class ScriptsProcessor(BaseProcessor):
    def process_row(self, row):
        logging.info(f"Processing Scripts row: {row}")
        document = {"headerName": row["header"], "categories": []}
        categories_dict = defaultdict(lambda: defaultdict(list))

        # Extract category & subcategory data dynamically
        for category in self.config["header_groups"]["Scripts"]["categories"]:
            for subCategory in self.config["header_groups"]["Scripts"]["subCategories"]:
                column_key = f"{category} - {subCategory}"
                if column_key in row:
                    year = row["year"]  # Take year directly from the input
                    value = row[column_key]  # Take corresponding value
                    categories_dict[category][subCategory].append({"year": year, "value": value})

        # Format final JSON structure
        for category, subCategories in categories_dict.items():
            category_obj = {"categoryName": category, "subCategories": []}
            for subName, yearly_data in subCategories.items():
                category_obj["subCategories"].append({"subCategoryName": subName, "yearlyData": yearly_data})
            document["categories"].append(category_obj)

        if document["categories"]:
            self.processed_documents.append(document)

# Processing Engine
def process_csv(csv_file, config):
    logging.info("Starting CSV Processing")
    df = pd.read_csv(csv_file)

    # Initialize processors for each group
    processors = {
        "PL_Information": PLInformationProcessor("PL_Information", config, "pl_information_output.json"),
        "Scripts": ScriptsProcessor("Scripts", config, "scripts_output.json")
    }

    # Process rows based on group
    for _, row in df.iterrows():
        group_name = row["header"]
        logging.info(f"Processing row under group {group_name}")
        if group_name in processors:
            processors[group_name].process_row(row)

    # Save results to JSON files
    for processor in processors.values():
        processor.save_to_file()

if __name__ == "__main__":
    csv_file = "input_data.csv"
    logging.info("Starting Main Execution")
    process_csv(csv_file, config)
