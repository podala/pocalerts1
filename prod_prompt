You are a senior Spark engineer building a production-grade PySpark data pipeline.

### ğŸ›  Use Case:
Build a modular Spark pipeline with:
- Source: Azure SQL via JDBC
- Destination: Azure Cosmos DB using bulk executor
- Transformations: Filtering, joins, aggregations (lightweight)
- SLA: Total runtime under 10 seconds
- Data volume: ~100Kâ€“500K rows per batch

---

### ğŸ“¦ Architectural Requirements:

Create the project using a clean, modular structure with a shared base class and strict separation of concerns:

1. `base.py`: A base class `BasePipelineComponent` that:
   - Initializes SparkSession with tuned config (AQE, Kryo, shuffle settings)
   - Initializes Python logging
   - Provides shared utilities to subclasses (logging,restart, Spark access)

2. `config.py`: Central config file with:
   - Spark config dictionary
   - JDBC config dictionary (partitioned read settings)
   - Cosmos DB config dictionary (including partition key, retry settings)

3. `reader.py`: A class `AzureSQLReader(BasePipelineComponent)` that:
   - Reads from Azure SQL using JDBC
   - Uses partitionColumn, fetchSize, numPartitions
   - Pushes filters and projections into the query
   - Logs partition read status

4. `transformer.py`: A class `SalesDataTransformer(BasePipelineComponent)` that:
   - Applies transformations with minimal shuffle
   - Uses broadcast join for small lookup table
   - Avoids UDFs, uses DataFrame API
   - each class as its own dataclass
   - Handles skew if detected (optional salting or adaptive strategies)

5. `writer.py`: A class `CosmosDBWriter(BasePipelineComponent)` that:
   - Writes to Cosmos DB using `ItemOverwrite` strategy
   - Enables bulk writes
   - Uses partition key (`/regionId`)
   - Handles retries and logs status

6. `main.py`: Main orchestration script that:
   - Instantiates each module class
   - Orchestrates reader â†’ transformer â†’ writer
   - Wraps entire flow in try/except with structured logging
   - Measures execution time and logs success/failure

---

### âœ… Technical Best Practices (Must Be Followed in Code):

- âœ… Push down filtering and projections to SQL query
- âœ… Use JDBC parallelism with partitionColumn
- âœ… Avoid collect(), toPandas(), groupByKey
- âœ… Minimize UDF usage
- âœ… Enable AQE and configure `spark.sql.shuffle.partitions`
- âœ… Use broadcast joins for small tables
- âœ… Use coalesce() instead of repartition() for small outputs
- âœ… Use efficient serialization (Kryo)
- âœ… Use structured logging via `logging` module
- âœ… Tune Cosmos DB write config (bulk, retry, partition key)
- âœ… Include clear comments on all optimizations
- âœ… Avoid hardcoded secrets â€” use placeholders or config
- âœ… Apply factory or builder patterns where appropriate
- âœ… Ensure every class is reusable and testable

---

### ğŸ“Š Spark UI & Runtime Validation Checklist (Output Required):

At the end, provide a checklist of:
- Spark UI metrics to monitor (shuffle read/write, stage skew, GC time)
- What to look for in SQL/Jobs/Stages tabs
- How to verify broadcast join, AQE benefits, and partitioning effectiveness

---

### ğŸ“ Expected Output Format:

1. `base.py` â€” base class
2. `config.py` â€” config holder
3. `reader.py` â€” JDBC reader class
4. `transformer.py` â€” transformation class with broadcast join
5. `writer.py` â€” Cosmos DB writer class
6. `main.py` â€” orchestrator with logging, timing, error handling

The final pipeline should be:
- Modular
- Reusable
- Low-latency
- Cloud-optimized
- Easily testable and maintainable

Generate full working code. Assume this runs on Azure Databricks or Synapse Spark.
