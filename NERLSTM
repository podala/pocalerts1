import numpy as np
import random
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, TimeDistributed, Bidirectional
from tf2crf import CRF
from sklearn.model_selection import train_test_split
import json
import pickle

# Step 1: Generate Sample Data
def generate_data(num_samples):
    team_names = ["Supervisors", "ProjectX", "AlphaTeam", "Zebra", "Champions"]
    user_names = ["Alice", "Bob", "Charlie", "David"]
    commands = [
        "Initialize team {} with {}",
        "Setup team {} with members {}",
        "Register team {} and include {}",
        "Form team {} and add {}"
    ]
    sentences = []
    annotations = []
    for _ in range(num_samples):
        team = random.choice(team_names)
        users = ', '.join(random.sample(user_names, random.randint(1, 3)))
        sentence = random.choice(commands).format(team, users)
        team_start = sentence.find(team)
        team_end = team_start + len(team)
        user_start = sentence.find(users)
        user_end = user_start + len(users)
        sentences.append(sentence)
        annotations.append([(team_start, team_end, 'TEAM_NAME'), (user_start, user_end, 'USER')])
    return sentences, annotations

sentences, annotations = generate_data(500)

# Step 2: Define the Model Architecture
def create_model(vocab_size, num_tags):
    input = Input(shape=(None,))
    model = Embedding(input_dim=vocab_size, output_dim=50, input_length=None)(input)
    model = Bidirectional(LSTM(units=64, return_sequences=True))(model)
    model = TimeDistributed(Dense(num_tags))(model)
    crf = CRF()
    output = crf(model)
    model = Model(input, output)
    model.compile(optimizer='adam', loss=crf.loss, metrics=[crf.accuracy])
    return model

# Step 3: Tokenization and Encoding
tokenizer = Tokenizer(num_words=5000, oov_token='UNK')
tokenizer.fit_on_texts(sentences)
X = tokenizer.texts_to_sequences(sentences)
X = pad_sequences(X, padding='post')

# Convert annotations to labels
def annotations_to_labels(annotations, length, num_tags):
    labels = np.zeros((length, num_tags))
    for start, end, tag in annotations:
        for i in range(start, end):
            labels[i, 0 if tag == 'TEAM_NAME' else 1] = 1
    return labels

num_tags = 2  # TEAM_NAME, USER
max_len = max(len(s) for s in X)
Y = [annotations_to_labels(ann, max_len, num_tags) for ann in annotations]
Y = np.array([pad_sequences([y], maxlen=max_len, padding='post')[0] for y in Y])

# Step 4: Training
model = create_model(5000, num_tags)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)
model.fit(X_train, Y_train, batch_size=32, epochs=10, validation_data=(X_test, Y_test))

# Step 5: Save Model
model.save("ner_model.h5")
with open('tokenizer.pkl', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)

# Step 6: Load Model
from tensorflow.keras.models import load_model
model = load_model("ner_model.h5", custom_objects={'CRF': CRF, 'loss': CRF.loss, 'accuracy': CRF.accuracy})

# Prediction Function
def predict(text):
    seq = tokenizer.texts_to_sequences([text])
    padded = pad_sequences(seq, maxlen=max_len)
    pred = model.predict(padded)
    return pred

# Example usage
text_input = json.dumps({"sentence": "Register team AlphaTeam and include Alice, Bob"})
loaded_text = json.loads(text_input)
output = predict(loaded_text["sentence"])

print("Input JSON:", text_input)
print("Output JSON:", json.dumps(output.tolist()))  # Output will be indices; convert to tag names as needed
