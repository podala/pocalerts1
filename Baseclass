import logging
import time
from abc import ABC, abstractmethod
from tenacity import retry, stop_after_attempt, wait_fixed, retry_if_exception_type
from writer.cosmos_writer import CosmosDBWriter

logger = logging.getLogger("BasePipeline")
logging.basicConfig(level=logging.INFO)

class BasePipeline(ABC):
    def __init__(self, spark, config):
        self.spark = spark
        self.config = config
        self.years = config["years"]
        self.ttl = config.get("ttl", 7776000)
        self.writer = CosmosDBWriter.get_instance(config["cosmos_config"])

    def extract(self):
        try:
            start = time.time()
            df = self.spark.read \
                .format("com.crealytics.spark.excel") \
                .option("header", "true") \
                .option("inferSchema", "true") \
                .load(self.config["input_path"])
            df = df.cache()
            row_count = df.count()
            elapsed = time.time() - start
            logger.info(f"‚úÖ Extracted {row_count} rows in {elapsed:.2f}s using {df.rdd.getNumPartitions()} partitions")
            return df
        except Exception as e:
            logger.exception("‚ùå Failed during extract phase")
            raise e

    @abstractmethod
    def transform(self, df):
        pass

    def serialize_to_json(self, df):
        years = self.years
        metrics_config = self.config["metrics_per_section"]

        def row_to_json(row):
            base_fields = {
                "id": row.get("Key"),
                "drugKey": row.get("Key"),
                "drugName": row.get("DrugName"),
                "clientName": row.get("clientName"),
                "opportunityId": row.get("opportunityId"),
                "therapeuticClass": row.get("TherapeuticClass"),
                "partitionKey": row.get("modelId"),
                "ttl": self.ttl,
            }

            section_data = {}
            for section, metrics in metrics_config.items():
                section_data[section] = {}
                for metric in metrics:
                    section_data[section][metric] = {
                        str(y): row.get(f"{section}_{metric}_{y}", 0.0) for y in years
                    }

            return {**base_fields, **section_data}

        return df.rdd.map(row_to_json)

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_fixed(5),
        retry=retry_if_exception_type(Exception)
    )
    def write(self, json_rdd):
        try:
            logger.info("üì§ Writing to Cosmos DB")
            self.writer.write(json_rdd)
            logger.info("‚úÖ Write completed")
        except Exception as e:
            logger.exception("‚ùå Write to Cosmos DB failed. Retrying...")
            raise e

    def run(self):
        try:
            logger.info("üöÄ Starting pipeline")
            df = self.extract()
            transformed = self.transform(df)
            json_rdd = self.serialize_to_json(transformed)
            self.write(json_rdd)
            logger.info("üèÅ Pipeline completed successfully")
        except Exception as e:
            logger.error("üí• Pipeline failed", exc_info=True)
