#!/usr/bin/env python3
"""
Group Runner (pure Python / pandas) — optimized for ~20k rows
-------------------------------------------------------------
• Reads full CSV (must include ndc + formulary + any rule fields)
• Loads rules JSON + static JSON
• Enforces: rules within a group have non-overlapping applies_to_years
• Evaluates each rule only on its own years
• Outputs ONE file per group:
    ndc, formulary, group, year1, year2, year3, year4, year5
• Also writes: status CSV (client, ndc, group, rule, status)
• Writes errors as JSONL only if any occur
• Logs: ERROR-only (quiet when successful)
"""

# --- project import bootstrap (so `engine` resolves no matter CWD/VS Code) ---
import os, sys
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# ---------------------------------------------------------------------------

from __future__ import annotations
import argparse, json, logging, tempfile
from typing import Any, Dict, List, Optional, Set, Tuple

import pandas as pd

# Engine: JSONLogic evaluator + rule executor
from engine.rule_executor import JSONLogicRuleExecutor
from engine.jsonlogic_core import validate_jsonlogic_node


# ============== Logging (ERROR-only) ==============
def set_error_only_logging() -> None:
    root = logging.getLogger()
    root.handlers.clear()
    root.setLevel(logging.ERROR)
    h = logging.StreamHandler(sys.stdout)
    h.setLevel(logging.ERROR)
    h.setFormatter(logging.Formatter("%(asctime)s | %(name)s | %(levelname)s | %(message)s"))
    root.addHandler(h)

log = logging.getLogger("GROUP_RUNNER")


# ============== Small helpers ==============
def to_float_or_none(x) -> Optional[float]:
    """Best-effort float coercion; None on blanks/None/errors (safe for CSV)."""
    try:
        if x is None:
            return None
        if isinstance(x, (int, float)):
            return float(x)
        s = str(x).strip()
        if s == "":
            return None
        return float(s)
    except Exception:
        return None


def write_csv_atomic(df: pd.DataFrame, out_path: str) -> None:
    """Atomic CSV write: temp file then replace."""
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    with tempfile.NamedTemporaryFile("w", delete=False, dir=os.path.dirname(out_path),
                                     suffix=".tmp", encoding="utf-8", newline="") as tmp:
        df.to_csv(tmp.name, index=False)
        tmp_path = tmp.name
    os.replace(tmp_path, out_path)


# ============== Ruleset validation & utilities ==============
def validate_ruleset(rules_data: Dict[str, Any], group: str) -> None:
    """Schema + node validation (raises on violation)."""
    years = rules_data.get("years")
    if not isinstance(years, list) or not years:
        raise ValueError("rules.years must be a non-empty list")
    years_n = len(years)

    rules = rules_data.get("rules", {})
    order = rules_data.get("rule_priority", [])
    if not order:
        raise ValueError("rules.rule_priority must be non-empty")
    for r in order:
        if r not in rules:
            raise ValueError(f"rule_priority contains unknown rule '{r}'")

    if not any(rules.get(r, {}).get("group") == group for r in order):
        raise ValueError(f"No rules for group '{group}' in rule_priority")

    # per-rule node validation (light)
    for rname, rdef in rules.items():
        if "group" not in rdef:
            raise ValueError(f"rule '{rname}' missing 'group'")
        applies = rdef.get("applies_to_years")
        if applies is not None:
            for y in applies:
                if not str(y).isdigit() or not (1 <= int(y) <= years_n):
                    raise ValueError(f"rule '{rname}' has invalid year index '{y}' (1..{years_n})")
        for k in ("conditions", "calculation", "else"):
            if rdef.get(k) is not None:
                validate_jsonlogic_node(rdef[k], path=f"$.{rname}.{k}")


def assert_disjoint_years(selected_rules: List[str], all_rules: Dict[str, Any], years_full: List[str]) -> None:
    """Fail fast if two rules claim the same year within the group."""
    used = {}
    for r in selected_rules:
        applies = all_rules[r].get("applies_to_years") or [str(i + 1) for i in range(len(years_full))]
        for y in applies:
            if y in used:
                raise ValueError(f"Rules '{used[y]}' and '{r}' both claim year index '{y}'. "
                                 "Each year must be produced by exactly one rule.")
            used[y] = r


def _vars_in(node: Any, acc: Set[str]) -> None:
    """Collect '$.foo' keys referenced in a JSONLogic node (raw; may contain '{year}')."""
    if node is None:
        return
    if isinstance(node, dict):
        if "var" in node or "path" in node:
            p = (node.get("path") or node.get("var") or "")
            if p.startswith("$."):
                acc.add(p[2:])  # strip leading '$.'
        for v in node.values():
            _vars_in(v, acc)
    elif isinstance(node, list):
        for v in node:
            _vars_in(v, acc)


def required_columns_from_rules(rules_data: Dict[str, Any], group: str,
                                years_full: List[str], id_col: str, formulary_col: str) -> Set[str]:
    """
    Build the set of CSV columns actually needed by this group:
      • expands '{year}' into 1..N for any var/path with that placeholder
      • always includes id_col and formulary_col
    """
    need_raw: Set[str] = set()
    for rname, rdef in rules_data["rules"].items():
        if rdef.get("group") != group:
            continue
        for key in ("conditions", "calculation", "else"):
            _vars_in(rdef.get(key), need_raw)

    expanded: Set[str] = set()
    for k in need_raw:
        if "{year}" in k:
            for i in range(1, len(years_full) + 1):
                expanded.add(k.replace("{year}", str(i)))
        else:
            expanded.add(k)

    expanded.add(id_col)
    expanded.add(formulary_col)
    return expanded


# ============== Core group run ==============
def run_group(
    df: pd.DataFrame,
    rules_data: Dict[str, Any],
    static_ctx: Dict[str, Any],
    group: str,
    *,
    id_col: str,
    formulary_col: str,
    client: str
) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[pd.DataFrame]]:
    """
    Execute all rules for a group and return:
      • final_df  : ndc, formulary, group, year1..yearN
      • status_df : client, ndc, group, rule, status
      • errors_df : (or None) ndc, group, rule, year_index, error
    """
    all_rules = rules_data["rules"]
    rule_order = rules_data["rule_priority"]
    years_full = rules_data.get("years", [])
    years_n = len(years_full) or 5

    selected = [r for r in rule_order if all_rules.get(r, {}).get("group") == group]
    if not selected:
        raise ValueError(f"No rules for group '{group}'")
    assert_disjoint_years(selected, all_rules, years_full)

    # Prepare executors (validate nodes once in constructor)
    executors: Dict[str, JSONLogicRuleExecutor] = {
        rname: JSONLogicRuleExecutor({**all_rules[rname], "name": rname})
        for rname in selected
    }
    applies_for: Dict[str, List[str]] = {
        rname: (all_rules[rname].get("applies_to_years") or [str(i + 1) for i in range(years_n)])
        for rname in selected
    }

    # Aggregators
    ndc_to_formulary: Dict[str, Any] = {}
    values: Dict[Tuple[str, str], Optional[float]] = {}  # (ndc, year_index) -> value
    status_rows: List[Dict[str, Any]] = []
    error_rows: List[Dict[str, Any]] = []

    # Input guards
    if id_col not in df.columns:
        raise ValueError(f"Input CSV missing required id column: {id_col}")
    if formulary_col not in df.columns:
        raise ValueError(f"Input CSV missing formulary column: {formulary_col}")

    # Fast path: iterate records (dicts), turn NaN->None once
    pdf = df.where(pd.notnull(df), None)
    for rowd in pdf.to_dict("records"):
        ndc = rowd.get(id_col)
        if ndc is None:
            continue
        ndc = str(ndc)
        if ndc not in ndc_to_formulary:
            ndc_to_formulary[ndc] = rowd.get(formulary_col)

        for rname in selected:
            execu = executors[rname]
            for yidx in applies_for[rname]:
                val, branch, err, _ms = execu.execute_row(rowd, static_ctx, yidx)
                if err:
                    error_rows.append({
                        "ndc": ndc, "group": group, "rule": rname,
                        "year_index": yidx, "error": err
                    })
                else:
                    values[(ndc, yidx)] = to_float_or_none(val)

            # mark status per ndc per rule
            status_rows.append({
                "client": client, "ndc": ndc, "group": group,
                "rule": rname, "status": "completed"
            })

    # Build final wide table
    out_rows: List[Dict[str, Any]] = []
    ndcs = sorted(ndc_to_formulary.keys())
    for ndc in ndcs:
        one = {"ndc": ndc, "formulary": ndc_to_formulary.get(ndc), "group": group}
        for i in range(1, years_n + 1):
            one[f"year{i}"] = values.get((ndc, str(i)))
        out_rows.append(one)

    final_df = pd.DataFrame(out_rows, columns=["ndc", "formulary", "group"] + [f"year{i}" for i in range(1, years_n + 1)])
    status_df = pd.DataFrame(status_rows, columns=["client", "ndc", "group", "rule", "status"])
    errors_df = pd.DataFrame(error_rows, columns=["ndc", "group", "rule", "year_index", "error"]) if error_rows else None

    return final_df, status_df, errors_df


# ============== CLI ==============
def main():
    set_error_only_logging()

    ap = argparse.ArgumentParser("Group Runner (pandas, optimized)")
    ap.add_argument("--input",  required=True, help="CSV input (must include NDC + formulary + rule fields)")
    ap.add_argument("--rules",  required=True, help="Rules JSON path")
    ap.add_argument("--static", required=True, help="Static JSON path")
    ap.add_argument("--group",  required=True, help="Group name (e.g., shifted_script)")
    ap.add_argument("--id-col", default="NDC", help="ID column in CSV (e.g., NDC)")
    ap.add_argument("--formulary-col", default="formulary", help="Formulary column in CSV")
    ap.add_argument("--client", default="ClientA", help="Client id/name for status")
    ap.add_argument("--outdir", default="outputs", help="Output directory")
    ap.add_argument("--prune-columns", action="store_true",
                    help="Prune DataFrame to only columns referenced by the group's rules (+ id/formulary)")
    args = ap.parse_args()

    # Load configs
    with open(args.rules, "r", encoding="utf-8") as f:
        rules_data = json.load(f)
    with open(args.static, "r", encoding="utf-8") as f:
        static_ctx = json.load(f)

    # Validate ruleset + group
    try:
        validate_ruleset(rules_data, args.group)
    except Exception as e:
        log.error(f"[RULES-ERROR] {e}")
        raise SystemExit(1)

    # Read CSV
    df = pd.read_csv(args.input)

    # Optional pruning (keeps id/formulary + all rule-referenced columns; expands {year})
    if args.prune_columns:
        years_full = rules_data.get("years", [])
        need = required_columns_from_rules(rules_data, args.group, years_full, args.id_col, args.formulary_col)
        keep = [c for c in df.columns if c in need]
        # Never drop id/formulary even if absent in need (already included above)
        df = df.loc[:, keep]  # if some referenced cols are missing, engine will use static/defaults

    # Run group
    final_df, status_df, errors_df = run_group(
        df, rules_data, static_ctx, args.group,
        id_col=args.id_col, formulary_col=args.formulary_col, client=args.client
    )

    # Write outputs (atomic for CSVs)
    os.makedirs(args.outdir, exist_ok=True)
    out_final  = os.path.join(args.outdir, f"{args.group}__final.csv")
    out_status = os.path.join(args.outdir, f"{args.group}__status.csv")
    out_errors = os.path.join(args.outdir, f"{args.group}__errors.jsonl")

    write_csv_atomic(final_df, out_final)
    write_csv_atomic(status_df, out_status)

    if errors_df is not None and not errors_df.empty:
        with open(out_errors, "w", encoding="utf-8") as f:
            for _, r in errors_df.iterrows():
                f.write(json.dumps({k: (None if pd.isna(v) else v) for k, v in r.items()}, ensure_ascii=False) + "\n")
    # Done (console quiet unless errors)

if __name__ == "__main__":
    main()

