import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence
from torchcrf import CRF
from sklearn.preprocessing import LabelEncoder
import numpy as np
import random

# Sample Data Generation
def generate_data(num_samples):
    team_names = ["Supervisors", "ProjectX", "AlphaTeam", "Zebra", "Champions"]
    user_names = ["Alice", "Bob", "Charlie", "David"]
    commands = [
        ["Initialize", "team", "with"],
        ["Setup", "team", "with", "members"],
        ["Register", "team", "and", "include"],
        ["Form", "team", "and", "add"]
    ]
    sentences = []
    tags = []
    for _ in range(num_samples):
        team = random.choice(team_names).split()
        users = random.sample(user_names, random.randint(1, 3))
        command = random.choice(commands)
        sentence = command + team + ["with"] + users
        tag = ['O'] * len(command) + ['B-TEAM'] + ['I-TEAM'] * (len(team) - 1) + ['O'] + ['B-PERSON'] + ['I-PERSON'] * (len(users) - 1)
        sentences.append(sentence)
        tags.append(tag)
    return sentences, tags

# Custom Dataset
class NERDataset(Dataset):
    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):
        self.sentences = [torch.tensor([word_to_ix[word] for word in sentence], dtype=torch.long) for sentence in sentences]
        self.tags = [torch.tensor([tag_to_ix[tag] for tag in tag_list], dtype=torch.long) for tag_list in tags]

    def __len__(self):
        return len(self.sentences)

    def __getitem__(self, idx):
        return self.sentences[idx], self.tags[idx]

def pad_collate(batch):
    sentences, tags = zip(*batch)
    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)
    tags_padded = pad_sequence(tags, batch_first=True, padding_value=-1)
    return sentences_padded, tags_padded

# Model
class BiLSTM_CRF(nn.Module):
    def __init__(self, vocab_size, tagset_size):
        super(BiLSTM_CRF, self).__init__()
        self.embedding = nn.Embedding(vocab_size, 50)
        self.lstm = nn.LSTM(50, 32, num_layers=1, bidirectional=True, batch_first=True)
        self.hidden2tag = nn.Linear(64, tagset_size)
        self.crf = CRF(tagset_size, batch_first=True)

    def forward(self, sentence):
        embeds = self.embedding(sentence)
        lstm_out, _ = self.lstm(embeds)
        lstm_feats = self.hidden2tag(lstm_out)
        return lstm_feats

    def loss(self, sentences, tags):
        feats = self.forward(sentences)
        return -self.crf(feats, tags)

# Training function
def train(model, device, train_loader, optimizer, epochs=10):
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for sentences, tags in train_loader:
            sentences, tags = sentences.to(device), tags.to(device)
            model.zero_grad()
            loss = model.loss(sentences, tags)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch: {epoch}, Loss: {total_loss / len(train_loader)}")

# Main
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    sentences, tags = generate_data(100)
    word_to_ix = {word: i+1 for i, word in enumerate(set(w for s in sentences for w in s))}
    word_to_ix['<PAD>'] = 0
    tag_to_ix = {tag: i for i, tag in enumerate(set(t for ts in tags for t in ts))}
    tag_to_ix['<PAD>'] = -1

    model = BiLSTM_CRF(len(word_to_ix), len(tag_to_ix)).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.01)

    dataset = NERDataset(sentences, tags, word_to_ix, tag_to_ix)
    train_loader = DataLoader(dataset, batch_size=10, shuffle=True, collate_fn=pad_collate)

    train(model, device, train_loader, optimizer)
