import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, f1_score
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, wordnet as wn
from textblob import TextBlob
import spacy
import nltk
from sklearn.decomposition import NMF
from transformers import T5Tokenizer, T5ForConditionalGeneration
import matplotlib.pyplot as plt
import networkx as nx

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load spaCy model for advanced NLP tasks
nlp = spacy.load("en_core_web_sm")

# Load the CSV file containing care agents' notes
df = pd.read_csv('care_agents_notes.csv')

# Sample 50,000 records
df_sample = df.sample(n=50000, random_state=42)

# Convert notes to strings and handle missing values
df_sample['notes'] = df_sample['notes'].astype(str).fillna('')

documents = df_sample['notes'].tolist()

# Preprocessing function
def preprocess(text):
    # Correct spelling
    text = str(TextBlob(text).correct())
    # Tokenize and lowercase
    tokens = word_tokenize(text.lower())
    # Remove stopwords and punctuation
    tokens = [word for word in tokens if word.isalnum() and word not in stopwords.words('english')]
    # Lemmatization
    tokens = [token.lemma_ for token in nlp(' '.join(tokens))]
    return ' '.join(tokens)

preprocessed_documents = [preprocess(doc) for doc in documents]

# Synonym expansion
def get_synonyms(word):
    synonyms = set()
    for syn in wn.synsets(word):
        for lemma in syn.lemmas():
            synonyms.add(lemma.name().replace('_', ' '))
    return synonyms

def expand_query(query):
    query_tokens = word_tokenize(query.lower())
    expanded_tokens = []
    for token in query_tokens:
        expanded_tokens.append(token)
        synonyms = get_synonyms(token)
        if synonyms:
            expanded_tokens.extend(synonyms)
    return ' '.join(expanded_tokens)

# Initialize TF-IDF
tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)

# Search function
def search(query):
    query = preprocess(query)
    expanded_query = expand_query(query)
    query_vec = tfidf_vectorizer.transform([expanded_query])
    similarities = cosine_similarity(query_vec, tfidf_matrix)
    sorted_indices = np.argsort(similarities[0])[::-1]
    return [(documents[idx], similarities[0][idx]) for idx in sorted_indices if similarities[0][idx] > 0]

# Validation and evaluation
from sklearn.model_selection import train_test_split

# Split documents into train and test sets
train_docs, test_docs = train_test_split(documents, test_size=0.2, random_state=42)

# Preprocess and vectorize training documents
train_preprocessed = [preprocess(doc) for doc in train_docs]
tfidf_vectorizer.fit(train_preprocessed)

# Function to evaluate the search engine
def evaluate_search_engine():
    y_true = []
    y_pred = []
    for doc in test_docs:
        search_results = search(doc)
        if doc in [result[0] for result in search_results[:3]]:  # Top-3 results
            y_true.append(1)
            y_pred.append(1)
        else:
            y_true.append(1)
            y_pred.append(0)

    accuracy = sum(np.array(y_true) == np.array(y_pred)) / len(y_true)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)

    print(f"Accuracy: {accuracy:.2f}")
    print(f"Precision: {precision:.2f}")
    print(f"Recall: {recall:.2f}")
    print(f"F1-Score: {f1:.2f}")

# Example search and evaluation
search_results = search("scheduled follow up")
print(search_results)

evaluate_search_engine()

# Summarization using T5
model_name = 't5-small'
t5_tokenizer = T5Tokenizer.from_pretrained(model_name)
t5_model = T5ForConditionalGeneration.from_pretrained(model_name)

summarized_documents = []
batch_size = 100  # Adjust batch size as needed
for i in range(0, len(documents), batch_size):
    batch_docs = documents[i:i + batch_size]
    for doc in batch_docs:
        if len(doc.split()) > 50:  # Ensure the document is long enough for summarization
            inputs = t5_tokenizer.encode("summarize: " + doc, return_tensors='pt', max_length=512, truncation=True)
            summary_ids = t5_model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)
            summary = t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summarized_documents.append(summary)
            print(f"Summary of document:\n{summary}\n")

# NMF topic modeling
nmf_model = NMF(n_components=5, random_state=1)
nmf_topics = nmf_model.fit_transform(tfidf_matrix)

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print(f"Topic {topic_idx}:")
        print(" ".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))

no_top_words = 10
tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()
display_topics(nmf_model, tfidf_feature_names, no_top_words)

# Graph visualization for similar word matching
def visualize_similar_words(query, top_n=10):
    query = preprocess(query)
    expanded_query = expand_query(query)
    query_tokens = set(expanded_query.split())

    G = nx.Graph()

    for token in query_tokens:
        G.add_node(token)

    for token in query_tokens:
        synonyms = get_synonyms(token)
        for synonym in synonyms:
            if synonym in query_tokens:
                G.add_edge(token, synonym)

    pos = nx.spring_layout(G)
    plt.figure(figsize=(12, 8))
    nx.draw(G, pos, with_labels=True, node_size=3000, node_color="skyblue", font_size=16, font_color="black", font_weight="bold", edge_color="gray")
    plt.title("Similar Words Matching for Query: '{}'".format(query))
    plt.show()

# Example visualization
visualize_similar_words("scheduled")
