import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
from torch.nn.utils.rnn import pad_sequence
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import nltk
from nltk.tokenize import word_tokenize
from collections import Counter

nltk.download('punkt')

class TextDataset(Dataset):
    def __init__(self, texts, vocab=None):
        self.texts = [word_tokenize(text.lower()) for text in texts]
        if vocab is None:
            word_counts = Counter(word for text in self.texts for word in text)
            self.vocab = {word: i + 1 for i, word in enumerate(word_counts)}
        else:
            self.vocab = vocab

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = [self.vocab.get(word, 0) for word in self.texts[idx]]
        return torch.tensor(tokens, dtype=torch.long)

def collate_batch(batch):
    text_list = pad_sequence(batch, padding_value=0, batch_first=True)
    return text_list

class LSTMEncoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(LSTMEncoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size + 1, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)

    def forward(self, text):
        embedded = self.embedding(text)
        output, (hidden, _) = self.lstm(embedded)
        return output.mean(dim=1)

def train_model(model, dataloader, epochs=10):
    optimizer = optim.Adam(model.parameters())
    criterion = nn.MSELoss()
    model.train()
    for epoch in range(epochs):
        total_loss = 0
        for texts in dataloader:
            optimizer.zero_grad()
            outputs = model(texts)
            loss = criterion(outputs, torch.zeros_like(outputs))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')

def generate_embeddings(model, dataset):
    model.eval()
    embeddings = []
    for text in DataLoader(dataset, batch_size=1, collate_fn=collate_batch):
        output = model(text).squeeze(0)  # Ensure the output is squeezed
        embeddings.append(output)
    embeddings = torch.stack(embeddings).detach().numpy()
    print("Generated embeddings shape:", embeddings.shape)
    return embeddings

def search_text(query, model, dataset, embeddings):
    query_tokens = torch.tensor([dataset.vocab.get(word, 0) for word in word_tokenize(query.lower())]).unsqueeze(0)
    query_embedding = model(query_tokens).detach().numpy()
    print("Query embedding shape:", query_embedding.shape)
    cosine_scores = cosine_similarity(query_embedding, embeddings).flatten()
    results = [(dataset.texts[i], cosine_scores[i]) for i in range(len(cosine_scores))]
    results.sort(key=lambda x: x[1], reverse=True)
    return results

# Main script execution
texts = [
    "Discharge plan must be set",
    "Need to reach member to monitor the discharge",
    "Member reached out for consultation",
    "Member reached out for program verification"
    "Follow discharge protocol",
    "I need to contact member for further instructions"
]

dataset = TextDataset(texts)
dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_batch)
vocab_size = len(dataset.vocab)
embedding_dim = 64
hidden_dim = 128
model = LSTMEncoder(vocab_size, embedding_dim, hidden_dim)
train_model(model, dataloader)

embeddings = generate_embeddings(model, dataset)

query = "Discharge"
results = search_text(query, model, dataset, embeddings)
for text, similarity in results:
    print(f'Text: {" ".join(text)}, Similarity: {similarity:.4f}')
